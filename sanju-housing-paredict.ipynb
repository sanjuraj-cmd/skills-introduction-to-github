# %% [code]
# Importing libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OrdinalEncoder
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

#loading_data
train=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')
test= pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')
train.drop('PoolArea',axis=1,inplace=True)
test.drop('PoolArea',axis=1,inplace=True)

#Creating functions
def missing_val_show(df):
    total_rows=len(df)
    missing_info = df.isnull().sum().reset_index()
    missing_info.columns = ['column_name', 'missing_count']
    missing_info['missing_percentage'] = (missing_info['missing_count'] / total_rows) * 100
    missing_info = missing_info[missing_info['missing_count'] > 0]
    print(missing_info)
    return missing_info
    

missing_info= missing_val_show(train)   
columns_to_drop = missing_info[missing_info['missing_percentage'] > 70]['column_name']
train = train.drop(columns=columns_to_drop)
test = test.drop(columns=columns_to_drop)


# Identify categorical and numerical columns
def col_type(df):
    # Identify categorical and numerical columns
    categorical_columns = df.select_dtypes(include=['object', 'category']).columns
    numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns 
    print("Categorical_cols: " + str(len(categorical_columns)))
    print("Numerical_cols: " + str(len(numerical_columns)))
    return categorical_columns, numerical_columns

categorical_columns, numerical_columns = col_type(train)

# Fill missing values
def fill_blank(df):
    for col in df.columns:
        if df[col].dtype in ['float64', 'int64']:
            df[col].fillna(train[col].median(), inplace=True)  # Or use mean
        elif df[col].dtype == 'object':
            df[col].fillna(df[col].mode()[0], inplace=True)

fill_blank(train)
missing_val_show(train)

numerical_df = train[numerical_columns]
categorical_df = train[categorical_columns]
# Calculate the correlation matrix
correlation = numerical_df.corr()

# Select columns with high correlation to 'SalePrice'
high_corr_cols = correlation['SalePrice'][correlation['SalePrice'].abs() > 0.5].index.tolist()
high_corr_cols.remove('SalePrice')  # Exclude the target variable itself
print("Highly correlated columns:", high_corr_cols)


for col in high_corr_cols:
    Q1 = train[col].quantile(0.25)  # 25th percentile
    Q3 = train[col].quantile(0.75)  # 75th percentile
    IQR = Q3 - Q1

    # Define lower and upper bounds
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Filter out rows outside bounds
    train = train[(train[col] >= lower_bound) & (train[col] <= upper_bound)]

print(f"Remaining rows train: {train.shape[0]}")
print(f"Remaining columns train: {train.shape[1]}")

print(f"Remaining rows test: {test.shape[0]}")
print(f"Remaining columns test: {test.shape[1]}")


# Identify skewed columns (optional: only apply to highly correlated ones)
skewness = numerical_df.skew()
skewed_cols = skewness[skewness.abs() > 0.75].index.tolist()
train[skewed_cols] = np.log1p(train[skewed_cols])
# Display skewed columns
print("Skewed columns:", skewed_cols)
# Set up the number of subplots
num_skewed = len(skewed_cols)
cols = 3  # Number of columns in the grid
rows = (num_skewed // cols) + (num_skewed % cols > 0)  # Calculate number of rows needed

# Create subplots
fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
axes = axes.flatten()  # Flatten the 2D array of axes to 1D for easy indexing

# Plot each skewed column
for i, col in enumerate(skewed_cols):
    sns.histplot(train[col], kde=True, bins=15, color='blue', ax=axes[i])
    axes[i].set_title(f"Distribution of {col} (Skewness: {numerical_df[col].skew():.2f})")
    axes[i].set_xlabel(col)
    axes[i].set_ylabel("Frequency")

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()  # Adjust layout to prevent overlap
plt.show()

# # Check unique values for each column
# for col in categorical_columns:
#     print(f"{col}: {train[col].unique()}")
#All te categorical columns can be encoded here as all are ordinal

#Categorical Encoding
from sklearn.preprocessing import OrdinalEncoder

ordinal_encoder = OrdinalEncoder()
train[categorical_columns] = ordinal_encoder.fit_transform(train[categorical_columns])
test[categorical_columns] = ordinal_encoder.fit_transform(test[categorical_columns])

fill_blank(train)
fill_blank(test)

categorical_columns, numerical_columns = col_type(train)

missing_info= missing_val_show(train)


#Feature Scaling
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
num_features = numerical_columns
train[num_features] = scaler.fit_transform(train[num_features])

from sklearn.model_selection import train_test_split
X = train.drop('SalePrice', axis=1)  # Features
y = train['SalePrice'] 
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# from sklearn.linear_model import LinearRegression

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)



# from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)


from xgboost import XGBRegressor

# Initialize the XGBoost Regressor
xgb_model = XGBRegressor(
    n_estimators=100,  # Number of trees (similar to Random Forest)
    learning_rate=0.1,  # Step size shrinkage to prevent overfitting
    max_depth=3,       # Maximum depth of a tree
    random_state=42    # Ensures reproducibility
)

# Fit the model to the training data
xgb_model.fit(X_train, y_train)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Predictions
y_pred_rf = rf_model.predict(X_val)
y_pred_lr= lr_model.predict(X_val)
y_pred_xg = xgb_model.predict(X_val)
# Metrics
print("MAE LR:", mean_absolute_error(y_val, y_pred_lr))
print("RMSE LR:", np.sqrt(mean_squared_error(y_val, y_pred_lr)))
print("R² Score LR:", r2_score(y_val, y_pred_lr))

print("MAE RF:", mean_absolute_error(y_val, y_pred_rf))
print("RMSE RF:", np.sqrt(mean_squared_error(y_val, y_pred_rf)))
print("R² Score RF:", r2_score(y_val, y_pred_rf))

print("MAE XGB:", mean_absolute_error(y_val, y_pred_xg))
print("RMSE XGB:", np.sqrt(mean_squared_error(y_val, y_pred_xg)))
print("R² Score XGB:", r2_score(y_val, y_pred_xg))

y_pred= xgb_model.predict(test)
y_pred

# Create a submission file
submission = pd.DataFrame({
    'Id': test['Id'],  # Use the test dataset's IDs
    'SalePrice': y_pred  # Predicted prices
})
submission.to_csv('submission.csv', index=False)